{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T07:00:50.224428Z",
     "start_time": "2023-09-29T07:00:50.139341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: How to train a classifier using Weak Supervision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by Anastasiia Sedova (GitHub: @anasedova)\n",
    "\n",
    "In this tutorial, we are going to train a spam detection classifier using weakly supervised data. \n",
    "\n",
    "The steps:\n",
    "- Collect training data\n",
    "- Annotate this data in a weakly supervised setting\n",
    "    - Create labeling functions\n",
    "    - *Match* the labeling functions to the data samples\n",
    "    - Aggregate the labels with different label aggregation techniques\n",
    "        - Majority Vote\n",
    "        - FABLE \n",
    "- train a logistic regression classifier using weak labels\n",
    "- train a logistic regresison classifier with SepLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T07:00:55.351474Z",
     "start_time": "2023-09-29T07:00:55.268545Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "from wrench.utils import set_seed\n",
    "from wrench.endmodel import EndClassifierModel\n",
    "from wrench._logging import LoggingHandler\n",
    "\n",
    "\n",
    "from snorkel.utils import probs_to_preds\n",
    "from utils import load_raw_spam_dataset\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T07:00:55.742326Z",
     "start_time": "2023-09-29T07:00:55.613825Z"
    }
   },
   "outputs": [],
   "source": [
    "# the path to the folder where our data is stored\n",
    "\n",
    "path_to_data = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T13:52:02.389214Z",
     "start_time": "2023-08-10T13:52:02.374441Z"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use for training is Spam Detection YouTube comments dataset \n",
    "[3]. \n",
    "\n",
    "- The dataset consists of comments that YouTube users left under different videos.\n",
    "- Each sample is a comment (i.e., a word, a sentence, or a couple of sentences).\n",
    "- 1,586 train samples, 120 dev samples, 250 test samples\n",
    "- There are 2 types of samples:\n",
    "    - HAM: comments relevant to the video (even very simple ones), or\n",
    "    - SPAM: irrelevant (often trying to advertise something) or inappropriate messages\n",
    "    \n",
    "<img src=\"../img/spam_detection.png\" width=\"800\"/>\n",
    "\n",
    "**NB! Original dataset is manually labeled, but we won't use these gold labels for model training! We will use the dataset as unlabeled one (and label it in a weakly-supervised fasion).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T13:17:10.218562Z",
     "start_time": "2023-09-27T13:17:10.070115Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only callable can be used as callback",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# load the YouTube dataset\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df_train, df_dev, df_test \u001B[38;5;241m=\u001B[39m \u001B[43mload_raw_spam_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mload_train_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Y_train = df_train[\"label\"].values\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Y_test = df_test[\"label\"].values\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/part_2_spam_clf/utils.py:21\u001B[0m, in \u001B[0;36mload_raw_spam_dataset\u001B[0;34m(load_train_labels, split_dev_valid)\u001B[0m\n\u001B[1;32m     19\u001B[0m df.columns = map(str.lower, df.columns)\n\u001B[1;32m     20\u001B[0m # Remove comment_id field\n\u001B[0;32m---> 21\u001B[0m df = df.drop(\"comment_id\", axis=1)\n\u001B[1;32m     22\u001B[0m # Add field indicating source video\n\u001B[1;32m     23\u001B[0m df[\"video\"] = [i] * len(df)\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[1;32m    310\u001B[0m     )\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:678\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    663\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    664\u001B[0m     dialect,\n\u001B[1;32m    665\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    674\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m    675\u001B[0m )\n\u001B[1;32m    676\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 678\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:581\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1268\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1265\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1266\u001B[0m         new_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(index)\n\u001B[0;32m-> 1268\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_currow \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m new_rows\n\u001B[1;32m   1272\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msqueeze \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mcolumns) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/core/frame.py:636\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    630\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[1;32m    631\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[1;32m    632\u001B[0m     )\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    635\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[0;32m--> 636\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmrecords\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmrecords\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/core/internals/construction.py:455\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[0;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[1;32m    452\u001B[0m     index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n\u001B[1;32m    454\u001B[0m \u001B[38;5;66;03m# no obvious \"empty\" int column\u001B[39;00m\n\u001B[0;32m--> 455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mmissing\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43many\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_integer_dtype(dtype):\n\u001B[1;32m    456\u001B[0m     nan_dtype: DtypeObj\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    459\u001B[0m         \u001B[38;5;66;03m# calling sanitize_array ensures we don't mix-and-match\u001B[39;00m\n\u001B[1;32m    460\u001B[0m         \u001B[38;5;66;03m#  NA dtypes\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/core/generic.py:10891\u001B[0m, in \u001B[0;36mNDFrame._add_numeric_operations.<locals>.any\u001B[0;34m(self, axis, bool_only, skipna, level, **kwargs)\u001B[0m\n\u001B[1;32m  10880\u001B[0m \u001B[38;5;129m@doc\u001B[39m(\n\u001B[1;32m  10881\u001B[0m     _bool_doc,\n\u001B[1;32m  10882\u001B[0m     desc\u001B[38;5;241m=\u001B[39m_any_desc,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  10889\u001B[0m )\n\u001B[1;32m  10890\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21many\u001B[39m(\u001B[38;5;28mself\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, bool_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, skipna\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, level\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m> 10891\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mNDFrame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43many\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbool_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/core/generic.py:10466\u001B[0m, in \u001B[0;36mNDFrame.any\u001B[0;34m(self, axis, bool_only, skipna, level, **kwargs)\u001B[0m\n\u001B[1;32m  10458\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21many\u001B[39m(\n\u001B[1;32m  10459\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m  10460\u001B[0m     axis: Axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  10464\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m  10465\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series \u001B[38;5;241m|\u001B[39m bool_t:\n\u001B[0;32m> 10466\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_logical_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m  10467\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43many\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnanops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnanany\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbool_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m  10468\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/core/generic.py:10449\u001B[0m, in \u001B[0;36mNDFrame._logical_func\u001B[0;34m(self, name, func, axis, bool_only, skipna, level, **kwargs)\u001B[0m\n\u001B[1;32m  10446\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_bool_data()\n\u001B[1;32m  10447\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_reduce_axis1(name, func, skipna\u001B[38;5;241m=\u001B[39mskipna)\n\u001B[0;32m> 10449\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reduce\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m  10450\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m  10451\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m  10452\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m  10453\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskipna\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m  10454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbool_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m  10455\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilter_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbool\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m  10456\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/pandas/core/series.py:4470\u001B[0m, in \u001B[0;36mSeries._reduce\u001B[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001B[0m\n\u001B[1;32m   4466\u001B[0m         kwd_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbool_only\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4467\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m   4468\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSeries.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not implement \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkwd_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4469\u001B[0m     )\n\u001B[0;32m-> 4470\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m np\u001B[38;5;241m.\u001B[39merrstate(\u001B[38;5;28mall\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   4471\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op(delegate, skipna\u001B[38;5;241m=\u001B[39mskipna, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_ufunc_config.py:436\u001B[0m, in \u001B[0;36m__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mexc_info):\n\u001B[1;32m    435\u001B[0m     seterr(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moldstate)\n\u001B[0;32m--> 436\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _Unspecified:\n\u001B[1;32m    437\u001B[0m         seterrcall(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moldcall)\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_ufunc_config.py:305\u001B[0m, in \u001B[0;36mseterrcall\u001B[0;34m(func)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mCallable):\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(func, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwrite\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m    304\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func\u001B[38;5;241m.\u001B[39mwrite, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mCallable)):\n\u001B[0;32m--> 305\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly callable can be used as callback\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    306\u001B[0m pyvals \u001B[38;5;241m=\u001B[39m umath\u001B[38;5;241m.\u001B[39mgeterrobj()\n\u001B[1;32m    307\u001B[0m old \u001B[38;5;241m=\u001B[39m geterrcall()\n",
      "\u001B[0;31mValueError\u001B[0m: Only callable can be used as callback"
     ]
    }
   ],
   "source": [
    "# load the YouTube dataset\n",
    "\n",
    "df_train, df_dev, df_test = load_raw_spam_dataset(load_train_labels=True)\n",
    "# Y_train = df_train[\"label\"].values\n",
    "# Y_test = df_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T13:17:10.636229Z",
     "start_time": "2023-09-27T13:17:10.527360Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data sample in the original dataset (i.e., a YouTube comment), we know:\n",
    "- comment's author,\n",
    "- date when the corresponding comment was left,\n",
    "- text of the sample,\n",
    "- gold manual label,\n",
    "- id of the YouTube video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T09:17:15.922279Z",
     "start_time": "2023-09-27T09:17:15.866145Z"
    }
   },
   "outputs": [],
   "source": [
    "# some examples of positive (=non-spam) samples, label id 0\n",
    "\n",
    "df_train.loc[df_train[\"label\"]==0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T09:17:18.497810Z",
     "start_time": "2023-09-27T09:17:18.441791Z"
    }
   },
   "outputs": [],
   "source": [
    "# some examples of negative (=spam) samples, label id 0\n",
    "\n",
    "df_train.loc[df_train[\"label\"]==1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T09:17:20.351538Z",
     "start_time": "2023-09-27T09:17:20.297051Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train[[\"text\", \"label\"]][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to start weak supervision! So, let's imagin the gold labels disappeared... \n",
    "\n",
    "<img src=\"../img/poof.jpg\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and here we are: there is some data we want to use for classifier training, but we don't have any labels and capacity/time/money/... for hiring annotators.\n",
    "\n",
    "But we can label this data with **weak supervision** :)\n",
    "\n",
    "<img src=\"../img/rainbow.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak Supervision\n",
    "\n",
    "A brief reminder how weak supervision works:\n",
    "1. We come up with some heuristic rules and transform these rules into labeling functions.\n",
    "2. We apply these labeling functions to the data and obtain weak labels.\n",
    "3. We use this weak labels to train a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the training samples we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_train.text[100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T21:04:15.013913Z",
     "start_time": "2023-08-09T21:04:15.007312Z"
    }
   },
   "source": [
    "## Task: formulate the rules that could annotate the training samples\n",
    "\n",
    "The questions that might help you: \n",
    "\n",
    "*What patterns are typical for spam YouTube comments? for non-spam comments?*\n",
    "\n",
    "*What rules might help to distinguish between spam and not-spam YouTube comments?*\n",
    "\n",
    "*What labeling functions do you think are productive and useful to annotate the YouTube comments?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules: \n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "5. ...\n",
    "6. ...\n",
    "7. ...\n",
    "8. ...\n",
    "9. ...\n",
    "10. ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My examples of rules: \n",
    "- \"check\"/\"check out\": if there is a collocation \"check out\" in the comment, most probably this comment is spam (and the comment author is promoting his/her channel)\n",
    "- \"subscribe\": same\n",
    "- \"my\": same\n",
    "- ...\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can be a rule?\n",
    "\n",
    "- Keyword searches: looking for specific words in a sentence\n",
    "- Pattern matching: looking for specific syntactical patterns\n",
    "- Third-party models: using an pre-trained model (usually a model for a different task than the one at hand)\n",
    "- ...\n",
    "- Crowdworker labels: treating each crowdworker as a black-box function that assigns labels to subsets of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules into labeling functions\n",
    "\n",
    "After we collected some rules, we transform them into labeling functions that could *label* the data sample - that is, assign it to one or another class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T09:17:26.234833Z",
     "start_time": "2023-09-27T09:17:26.188658Z"
    }
   },
   "outputs": [],
   "source": [
    "# an example of LF based on a keyword \"check out\"\n",
    "\n",
    "def check_out(x):\n",
    "    return 1 if \"check out\" in x.text.lower() else -1\n",
    "\n",
    "# meaning the sample will be assigned to class 1 (=SPAM) if there is a \"check out\" expression in the comment, \n",
    "# otherwise to class 0 (=non-SPAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T09:17:27.103055Z",
     "start_time": "2023-09-27T09:17:27.049993Z"
    }
   },
   "outputs": [],
   "source": [
    "# an example of LF based on a key word \"please\"\n",
    "\n",
    "def check(x):\n",
    "    return 1 if \"please\" in x.text.lower() else -1\n",
    "\n",
    "# meaning the sample will be assigned to class 1 (=SPAM) if there is a \"please\" expression in the comment, \n",
    "# otherwise to class 0 (=non-SPAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T22:01:27.414881Z",
     "start_time": "2023-08-09T22:01:26.383285Z"
    }
   },
   "source": [
    "### Labeling functions we are going to use\n",
    "\n",
    "In this tutorial, we are going to use the labeling functions created by [Snorkel team](https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/01_spam_tutorial.ipynb), which are: \n",
    "\n",
    "\n",
    "1. keyword **\"my\"** (to detect spam comments like \"my channel\", \"my video\", etc)\n",
    "2. keyword **\"subscribe\"** (to detect spam comments that ask users to subscribe to some channel)\n",
    "3. keyword **\"http\"** (to detect spam comments that link to other channels)\n",
    "4. keyword **\"please\"/\"plz\"** (to detect spam comments that make requests rather than commenting)\n",
    "5. keyword **\"song\"** (to detect non-spam comments that actually talk about the video's content)\n",
    "6. regex **\"check_out\"** (to detect spam comments like \"check out this channel\", etc)\n",
    "7. **short comment** (non-spam comments are often short, such as 'cool video!')\n",
    "8. **mentioning specific people** and are **short** (using SpaCy library; non-spam comments usually mention some people)\n",
    "9. **polarity** (using TextBlob library; if polarity > 0.9, it is most probably a non-spam message)\n",
    "10. **subjectivity** (using TextBlob library; if subjectivity >= 0.5, it is most probably a non-spam message)\n",
    "\n",
    "(We are not going into details of the labeling process here now - you will hear more about it from my colleagues later). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed data\n",
    "\n",
    "The resulted annotations can be saved in the following format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T09:17:30.745298Z",
     "start_time": "2023-09-27T09:17:30.693320Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/youtube/train.json\") as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "train_data[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T14:38:12.461055Z",
     "start_time": "2023-08-10T14:38:12.445819Z"
    }
   },
   "source": [
    "The structure of the processed data is the following: \n",
    "- data.text: the text of the sample\n",
    "- label: gold label obtained by manual annotation\n",
    "- weak_labels: the results of annotation by labeling functions. \n",
    "    - -1: the corresponding labeling function did not match\n",
    "    - 0: the labeling function matched and assigned this sample to class 0 (non-spam class in our case)\n",
    "    - 1: the labeling function matched and assigned this sample to class 1 (spam class in our case)\n",
    "\n",
    "So, for the sample #1:\n",
    "(*if your like drones, plz subscribe to Kamal Tayara. He takes videos with  his drone that are absolutely beautiful.\\ufeff*)\n",
    "\n",
    "- labeling functions 1, 3, 5, 6, 7, 8, 9 did not match\n",
    "- labeling functions 2 (a key word *subscribe*) & 4 (a key word *plz*) matched and assigned this sample to the class 1\n",
    "- labeling function 10 (subjectivity score > 0.5) matched and assigned this sample to the class 0\n",
    "\n",
    "**Next step: how to turn these annotations into weak labels to train a classifier with them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T15:48:13.475728Z",
     "start_time": "2023-08-10T15:48:13.094259Z"
    }
   },
   "source": [
    "## Weak labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T15:48:15.062358Z",
     "start_time": "2023-08-10T15:48:15.049170Z"
    }
   },
   "source": [
    "There are different *label models* that calculate the weak labels based on labeling functions annotations. In this tutorial, we are going to try two of them: \n",
    "\n",
    "- **Majority Vote** (intuitive and straightforward)\n",
    "- **FABLE** [1] (most recent and well-performing)\n",
    "\n",
    "For label calculation and model training we will use a weakly supervised framework called [Wrench](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiRmYabjOGAAxW1h_0HHQt3COQQFnoECA4QAQ&url=https%3A%2F%2Fgithub.com%2FJieyuZ2%2Fwrench&usg=AOvVaw3EWVM0icLVHENbUv51USa_&opi=89978449) [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrench dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we transform our data into a Wrench-specific dataset.\n",
    "\n",
    "We can encode the data with TF-IDF features... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T09:24:47.708534Z",
     "start_time": "2023-09-27T09:24:47.456607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-29 07:01:10 - loading data from data/youtube/train.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7a566db5074819ad5ca890bb1e7432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-29 07:01:10 - loading data from data/youtube/valid.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2f2ebf69484f4f982b0ac6e79cde18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-29 07:01:10 - loading data from data/youtube/test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c738bf8b1f4363b88d3cc18b405c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "_unique_dispatcher() missing 1 required keyword-only argument: 'equal_nan'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# TF-IDF features\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwrench\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[0;32m----> 5\u001B[0m train_data_tfidf, valid_data_tfidf, test_data_tfidf \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_to_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m     \u001B[49m\u001B[38;5;66;43;03m# path to the folder where the dataset is stored\u001B[39;49;00m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myoutube\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m         \u001B[49m\u001B[38;5;66;43;03m# name of the dataset\u001B[39;49;00m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextract_feature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# we want to encode out data ...\u001B[39;49;00m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextract_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtfidf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# ... with TF-IDF features (other predefined options are 'sentence_transformer', 'bert')\u001B[39;49;00m\n\u001B[1;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/wrench/dataset/__init__.py:58\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(data_home, dataset, dataset_type, extract_feature, extract_fn, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m test_data \u001B[38;5;241m=\u001B[39m dataset_class(path\u001B[38;5;241m=\u001B[39mdataset_path, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m extract_feature \u001B[38;5;129;01mand\u001B[39;00m (dataset_class \u001B[38;5;241m!=\u001B[39m BaseSeqDataset):\n\u001B[0;32m---> 58\u001B[0m     extractor_fn \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mextract_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_extractor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m     valid_data\u001B[38;5;241m.\u001B[39mextract_feature(extract_fn\u001B[38;5;241m=\u001B[39mextractor_fn, return_extractor\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     60\u001B[0m     test_data\u001B[38;5;241m.\u001B[39mextract_feature(extract_fn\u001B[38;5;241m=\u001B[39mextractor_fn, return_extractor\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/wrench/dataset/basedataset.py:141\u001B[0m, in \u001B[0;36mBaseDataset.extract_feature\u001B[0;34m(self, extract_fn, return_extractor, cache_name, force, normalize, **kwargs)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;241m=\u001B[39m extract_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexamples)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 141\u001B[0m     extractor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_feature_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mextract_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_extractor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_extractor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m normalize:\n\u001B[1;32m    143\u001B[0m         features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/wrench/dataset/dataset.py:57\u001B[0m, in \u001B[0;36mTextDataset.extract_feature_\u001B[0;34m(self, extract_fn, return_extractor, device, model_name, feature, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m     data, extractor \u001B[38;5;241m=\u001B[39m bag_of_words_extractor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexamples, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m extract_fn \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtfidf\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 57\u001B[0m     data, extractor \u001B[38;5;241m=\u001B[39m \u001B[43mtf_idf_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m extract_fn \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence_transformer\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     59\u001B[0m     data, extractor \u001B[38;5;241m=\u001B[39m sentence_transformer_extractor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexamples,\n\u001B[1;32m     60\u001B[0m                                                      device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m     61\u001B[0m                                                      model_name\u001B[38;5;241m=\u001B[39mmodel_name,\n\u001B[1;32m     62\u001B[0m                                                      \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/wrench/dataset/utils.py:186\u001B[0m, in \u001B[0;36mtf_idf_extractor\u001B[0;34m(data, **kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m], data))\n\u001B[1;32m    185\u001B[0m tfidf_transformer \u001B[38;5;241m=\u001B[39m TfidfVectorizer(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 186\u001B[0m tfidf \u001B[38;5;241m=\u001B[39m \u001B[43mtfidf_transformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtoarray()\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextractor\u001B[39m(data: List[Dict]):\n\u001B[1;32m    189\u001B[0m     corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m], data))\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1847\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[1;32m   1846\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit_transform(raw_documents)\n\u001B[0;32m-> 1847\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tfidf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1848\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[1;32m   1849\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mtransform(X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1451\u001B[0m, in \u001B[0;36mTfidfTransformer.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m   1448\u001B[0m     \u001B[38;5;66;03m# log+1 instead of log makes sure terms with zero idf don't get\u001B[39;00m\n\u001B[1;32m   1449\u001B[0m     \u001B[38;5;66;03m# suppressed entirely.\u001B[39;00m\n\u001B[1;32m   1450\u001B[0m     idf \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(n_samples \u001B[38;5;241m/\u001B[39m df) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1451\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_idf_diag \u001B[38;5;241m=\u001B[39m \u001B[43msp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiags\u001B[49m\u001B[43m(\u001B[49m\u001B[43midf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffsets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1452\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mn_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_features\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1453\u001B[0m \u001B[43m                              \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1454\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/scipy/sparse/_construct.py:196\u001B[0m, in \u001B[0;36mdiags\u001B[0;34m(diagonals, offsets, shape, format, dtype)\u001B[0m\n\u001B[1;32m    190\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    191\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiagonal length (index \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m at offset \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m) does not \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    192\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124magree with matrix size (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    193\u001B[0m                 j, \u001B[38;5;28mlen\u001B[39m(diagonal), offset, m, n)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    194\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdia_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_arr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffsets\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39masformat(\u001B[38;5;28mformat\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/scipy/sparse/_dia.py:155\u001B[0m, in \u001B[0;36mdia_matrix.__init__\u001B[0;34m(self, arg1, shape, dtype, copy)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffsets):\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumber of diagonals (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    152\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdoes not match the number of offsets (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    153\u001B[0m             \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffsets)))\n\u001B[0;32m--> 155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moffsets\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffsets):\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moffset array contains duplicate values\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m<__array_function__ internals>:4\u001B[0m, in \u001B[0;36munique\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: _unique_dispatcher() missing 1 required keyword-only argument: 'equal_nan'"
     ]
    }
   ],
   "source": [
    "# TF-IDF features\n",
    "\n",
    "from wrench.dataset import load_dataset\n",
    "\n",
    "train_data_tfidf, valid_data_tfidf, test_data_tfidf = load_dataset(\n",
    "    path_to_data,     # path to the folder where the dataset is stored\n",
    "    \"youtube\",         # name of the dataset\n",
    "    extract_feature=True,      # we want to encode out data ...\n",
    "    extract_fn='tfidf'        # ... with TF-IDF features (other predefined options are 'sentence_transformer', 'bert')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or with BERT features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.461548Z",
     "start_time": "2023-09-26T15:03:59.660186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bert features\n",
    "\n",
    "train_data, valid_data, test_data = load_dataset(\n",
    "    path_to_data,       # path to the folder where the dataset is stored\n",
    "    \"youtube\",    # name of the dataset\n",
    "    extract_feature=True,      # we want to encode out data ...\n",
    "    extract_fn='bert',        # ... with bert embeddings\n",
    "    model_name='bert-base-cased',      # the name of the bert model\n",
    "    cache_name='bert'     # load it from cache if there are cached files \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what's inside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.551687Z",
     "start_time": "2023-09-26T15:07:52.455772Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# the format of the train_data, valida_data, and test_data now is: wrench.dataset.dataset.TextDataset\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain_data\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# the format of the train_data, valida_data, and test_data now is: wrench.dataset.dataset.TextDataset\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.628013Z",
     "start_time": "2023-09-26T15:07:52.545875Z"
    }
   },
   "outputs": [],
   "source": [
    "# how many classes are there in the dataset?\n",
    "\n",
    "train_data.n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.651490Z",
     "start_time": "2023-09-26T15:07:52.590584Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# how many labeling functions are there in the dataset?\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain_data\u001B[49m\u001B[38;5;241m.\u001B[39mn_lf\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# how many labeling functions are there in the dataset?\n",
    "\n",
    "train_data.n_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.713799Z",
     "start_time": "2023-09-26T15:07:52.633923Z"
    }
   },
   "outputs": [],
   "source": [
    "# what is the class_id to class correspondence?\n",
    "\n",
    "train_data.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.761577Z",
     "start_time": "2023-09-26T15:07:52.686981Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# how do the samples look like?\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain_data\u001B[49m\u001B[38;5;241m.\u001B[39mexamples[:\u001B[38;5;241m10\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# how do the samples look like?\n",
    "\n",
    "train_data.examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.782930Z",
     "start_time": "2023-09-26T15:07:52.730593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.7608848 ,  0.4292751 ,  0.99990165, ...,  0.99997175,\n",
       "        -0.7515034 ,  0.9910971 ],\n",
       "       [-0.7922811 ,  0.47435865,  0.9999021 , ...,  0.9999663 ,\n",
       "        -0.7555875 ,  0.9860616 ],\n",
       "       [-0.71461093,  0.4088342 ,  0.9997614 , ...,  0.9999327 ,\n",
       "        -0.5964781 ,  0.98014975],\n",
       "       ...,\n",
       "       [-0.7311086 ,  0.40834075,  0.9998686 , ...,  0.99996996,\n",
       "        -0.6895463 ,  0.9860986 ],\n",
       "       [-0.606766  ,  0.48277682,  0.99963987, ...,  0.9999085 ,\n",
       "        -0.89470583,  0.97945964],\n",
       "       [-0.7177589 ,  0.49044982,  0.9998655 , ...,  0.999958  ,\n",
       "        -0.82362586,  0.9884726 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how do the encoded samples look like?\n",
    "\n",
    "print(type(train_data.features))\n",
    "train_data.features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.859104Z",
     "start_time": "2023-09-26T15:07:52.781222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the weak annotations produced by labeling functions?\n",
    "\n",
    "train_data.weak_labels[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T21:57:30.953595Z",
     "start_time": "2023-08-09T21:57:30.941063Z"
    }
   },
   "source": [
    "### Majority Vote\n",
    "\n",
    "The simplest and most straightforward method to calculate labels from the noisy annotations is **majority voting** - a decision-making method where the option with the most votes is chosen. It's like asking a group of people to pick a movie, and the one that gets the most hands raised wins. \n",
    "\n",
    "In our case, each labeling function produces a *vote*; the most voted class is selected as a sample label. All ties are broken randomly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:  write your own majority vote function\n",
    "- Input: the weak annotations produced by labeling functions (stored in weak_labels field of wrench dataset objects)\n",
    "- Output: labels\n",
    "\n",
    "Before you start programming, think about possible bottlenecks: \n",
    "- what if a sample obtains equal amount of votes for some class?\n",
    "- what if there are no votes for a sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:07:52.873939Z",
     "start_time": "2023-09-26T15:07:52.824724Z"
    }
   },
   "outputs": [],
   "source": [
    "# todo\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def majority_vote(weak_annotations):\n",
    "    # calculate labels with majority vote \n",
    "    # output should be a numpy array\n",
    "    return labels\n",
    "\n",
    "labels_mv = majority_vote(train_data.weak_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ready solution to aggregate the weak labels with majority vote is already included to the Wrench framework:`MajorityVoting` label model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:08:14.170066Z",
     "start_time": "2023-09-26T15:08:07.434101Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize and fit the majority vote label model from the Wrench framework\n",
    "\n",
    "from wrench.labelmodel import MajorityVoting\n",
    "\n",
    "label_model = MajorityVoting()\n",
    "label_model.fit(dataset_train=train_data, dataset_valid=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:08:20.979011Z",
     "start_time": "2023-09-26T15:08:20.897840Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate weak labels \n",
    "\n",
    "soft_label_mv = label_model.predict_proba(train_data)    # soft label as probabilities across all classes\n",
    "hard_label_mv = probs_to_preds(soft_label_mv)               # hard labels as the most probable classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of jupyter_client.provisioning.provisioner_base failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 496, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 393, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 361, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 319, in update_instances\n",
      "    object.__setattr__(ref, \"__class__\", new)\n",
      "TypeError: can't apply this __setattr__ to type object\n",
      "]\n",
      "[autoreload of numpy.core.overrides failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/overrides.py\", line 32, in <module>\n",
      "    add_docstring(\n",
      "RuntimeError: implement_array_function method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.core.multiarray failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/multiarray.py\", line 16, in <module>\n",
      "    from ._multiarray_umath import (\n",
      "ImportError: cannot import name 'from_dlpack' from 'numpy.core._multiarray_umath' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)\n",
      "]\n",
      "[autoreload of numpy.core._asarray failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 11, in <module>\n",
      "    from .multiarray import array, asanyarray\n",
      "ImportError: cannot import name 'asanyarray' from 'numpy.core.multiarray' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/multiarray.py)\n",
      "]\n",
      "[autoreload of numpy.core._methods failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_methods.py\", line 11, in <module>\n",
      "    from numpy.core.multiarray import asanyarray\n",
      "ImportError: cannot import name 'asanyarray' from 'numpy.core.multiarray' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/multiarray.py)\n",
      "]\n",
      "[autoreload of numpy.core.fromnumeric failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 13, in <module>\n",
      "    from .multiarray import asarray, array, asanyarray, concatenate\n",
      "ImportError: cannot import name 'asarray' from 'numpy.core.multiarray' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/multiarray.py)\n",
      "]\n",
      "[autoreload of numpy.core.shape_base failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/shape_base.py\", line 11, in <module>\n",
      "    from .multiarray import array, asanyarray, normalize_axis_index\n",
      "ImportError: cannot import name 'asanyarray' from 'numpy.core.multiarray' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/multiarray.py)\n",
      "]\n",
      "[autoreload of numpy.core.numeric failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/numeric.py\", line 10, in <module>\n",
      "    from .multiarray import (\n",
      "ImportError: cannot import name 'asarray' from 'numpy.core.multiarray' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/multiarray.py)\n",
      "]\n",
      "[autoreload of numpy.core._add_newdocs failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_add_newdocs.py\", line 912, in <module>\n",
      "    add_newdoc('numpy.core.multiarray', 'asarray',\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/function_base.py\", line 521, in add_newdoc\n",
      "    new = getattr(__import__(place, globals(), {}, [obj]), obj)\n",
      "AttributeError: module 'numpy.core.multiarray' has no attribute 'asarray'\n",
      "]\n",
      "[autoreload of numpy.core failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/__init__.py\", line 100, in <module>\n",
      "    from . import _add_newdocs_scalars\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_add_newdocs_scalars.py\", line 243, in <module>\n",
      "    add_newdoc('numpy.core.numerictypes', \"integer\", ('is_integer',\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/function_base.py\", line 526, in add_newdoc\n",
      "    _add_docstring(getattr(new, attr), docstring.strip(), warn_on_python)\n",
      "AttributeError: type object 'numpy.integer' has no attribute 'is_integer'\n",
      "]\n",
      "[autoreload of numpy.linalg.linalg failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/linalg/linalg.py\", line 85, in <module>\n",
      "    _linalg_error_extobj = _determine_error_states()\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/linalg/linalg.py\", line 78, in _determine_error_states\n",
      "    with errstate(invalid='call', over='ignore',\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_ufunc_config.py\", line 436, in __enter__\n",
      "    if self.call is not _Unspecified:\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_ufunc_config.py\", line 305, in seterrcall\n",
      "    raise ValueError(\"Only callable can be used as callback\")\n",
      "ValueError: Only callable can be used as callback\n",
      "]\n",
      "[autoreload of numpy.matrixlib failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/matrixlib/__init__.py\", line 6, in <module>\n",
      "    __all__ = defmatrix.__all__\n",
      "NameError: name 'defmatrix' is not defined\n",
      "]\n",
      "[autoreload of numpy.lib.npyio failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 17, in <module>\n",
      "    from numpy.core._multiarray_umath import _load_from_filelike\n",
      "ImportError: cannot import name '_load_from_filelike' from 'numpy.core._multiarray_umath' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)\n",
      "]\n",
      "[autoreload of numpy.lib failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/lib/__init__.py\", line 44, in <module>\n",
      "    __all__ += type_check.__all__\n",
      "NameError: name 'type_check' is not defined\n",
      "]\n",
      "[autoreload of numpy.random._pickle failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/random/_pickle.py\", line 3, in <module>\n",
      "    from ._pcg64 import PCG64, PCG64DXSM\n",
      "ImportError: cannot import name 'PCG64DXSM' from 'numpy.random._pcg64' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-darwin.so)\n",
      "]\n",
      "[autoreload of numpy.random failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/random/__init__.py\", line 187, in <module>\n",
      "    from ._pcg64 import PCG64, PCG64DXSM\n",
      "ImportError: cannot import name 'PCG64DXSM' from 'numpy.random._pcg64' (/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-darwin.so)\n",
      "]\n",
      "[autoreload of numpy.ma.core failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/ma/core.py\", line 8239, in <module>\n",
      "    frombuffer = _convert2ma(\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/ma/core.py\", line 8151, in __init__\n",
      "    self.__doc__ = self.getdoc(np_ret, np_ma_ret)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/ma/core.py\", line 8159, in getdoc\n",
      "    doc = self._replace_return_type(doc, np_ret, np_ma_ret)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/ma/core.py\", line 8184, in _replace_return_type\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to replace `out : ndarray` with `out: MaskedArray`. The documentation string for return type, out : ndarray, is not found in the docstring for `np.frombuffer`. Fix the docstring for `np.frombuffer` or update the expected string for return type.\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/__init__.py\", line 225, in <module>\n",
      "    core.getlimits._register_known_types()\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/getlimits.py\", line 224, in _register_known_types\n",
      "    with numeric.errstate(all='ignore'):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_ufunc_config.py\", line 436, in __enter__\n",
      "    if self.call is not _Unspecified:\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/core/_ufunc_config.py\", line 305, in seterrcall\n",
      "    raise ValueError(\"Only callable can be used as callback\")\n",
      "ValueError: Only callable can be used as callback\n",
      "]\n",
      "[autoreload of numpy.testing._private.utils failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py\", line 53, in <module>\n",
      "    HAS_LAPACK64 = numpy.linalg.lapack_lite._ilp64\n",
      "AttributeError: module 'numpy.linalg' has no attribute 'lapack_lite'\n",
      "]\n",
      "[autoreload of numpy.testing failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/numpy/testing/__init__.py\", line 17, in <module>\n",
      "    __all__ = _private.utils.__all__ + ['TestCase', 'run_module_suite']\n",
      "NameError: name '_private' is not defined\n",
      "]\n",
      "[autoreload of utils failed: Traceback (most recent call last):\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 916, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/asedova/PycharmProjects/WS_tutorial/part_2_spam_clf/utils.py\", line 51\n",
      "    if __name__ == '__main__':\n",
      "    ^\n",
      "IndentationError: expected an indented block\n",
      "]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hard_label_mv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mhard_label_mv\u001B[49m\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[0;31mNameError\u001B[0m: name 'hard_label_mv' is not defined"
     ]
    }
   ],
   "source": [
    "hard_label_mv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 sentences, their weak annotations, and the weak labels obtained with majority voting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:08:24.167642Z",
     "start_time": "2023-09-26T15:08:24.112139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'pls http://www10.vakinha.com.br/VaquinhaE.aspx?e=313327 help me get vip gun  cross fire al\\ufeff'},\n",
       " {'text': 'if your like drones, plz subscribe to Kamal Tayara. He takes videos with  his drone that are absolutely beautiful.\\ufeff'},\n",
       " {'text': 'go here to check the views :3\\ufeff'},\n",
       " {'text': 'Came here to check the views, goodbye.\\ufeff'},\n",
       " {'text': 'i am 2,126,492,636 viewer :D\\ufeff'},\n",
       " {'text': 'https://www.facebook.com/teeLaLaLa\\ufeff'},\n",
       " {'text': 'imagine if this guy put adsense on with all these views... u could pay ur  morgage\\ufeff'},\n",
       " {'text': 'Follow me on Twitter @mscalifornia95\\ufeff'},\n",
       " {'text': 'Can we reach 3 billion views by December 2014? \\ufeff'},\n",
       " {'text': 'Follow 4 Follow                           @ VaahidMustafic Like 4 Like \\ufeff'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:08:27.263925Z",
     "start_time": "2023-09-26T15:08:27.207178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, -1, 1, -1, -1, -1, -1, -1, -1, -1],\n",
       " [-1, 1, -1, 1, -1, -1, -1, -1, -1, 0],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       " [-1, -1, 1, -1, -1, -1, 0, -1, -1, -1],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.weak_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:08:29.585852Z",
     "start_time": "2023-09-26T15:08:29.447845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_label_mv[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:08:32.403012Z",
     "start_time": "2023-09-26T15:08:32.344713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_label_mv[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FABLE \n",
    "\n",
    "Fable [1] is a label model where noisy labels are inferred not only based on the labeling functions' votes, but also using the instance features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:10:06.230196Z",
     "start_time": "2023-09-26T15:10:02.434766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values included: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                             | 2/1000 [00:18<2:36:20,  9.40s/iter]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize and apply the fable model\n",
    "from wrench.labelmodel import Fable\n",
    "\n",
    "label_model = Fable(kernel_function=None, num_groups=10)\n",
    "_ = label_model.fit(dataset_train=train_data, dataset_valid=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:10:13.120855Z",
     "start_time": "2023-09-26T15:10:09.497628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                             | 2/1000 [00:18<2:32:54,  9.19s/iter]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate labels\n",
    "soft_label_fable = label_model.predict_proba(train_data)\n",
    "hard_label_fable = probs_to_preds(soft_label_fable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:10:16.193173Z",
     "start_time": "2023-09-26T15:10:16.134392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54772653, 0.45227347],\n",
       "       [0.04384498, 0.95615502],\n",
       "       [0.56616534, 0.43383466],\n",
       "       [0.55068925, 0.44931075],\n",
       "       [0.55410647, 0.44589353],\n",
       "       [0.87257854, 0.12742146],\n",
       "       [0.53917847, 0.46082153],\n",
       "       [0.5582726 , 0.4417274 ],\n",
       "       [0.51587386, 0.48412614],\n",
       "       [0.54213748, 0.45786252]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_label_fable[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:10:18.034452Z",
     "start_time": "2023-09-26T15:10:17.979782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_label_fable[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:10:20.555067Z",
     "start_time": "2023-09-26T15:10:20.490550Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier with majorty vote hard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:10:27.927925Z",
     "start_time": "2023-09-26T15:10:22.078808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.908"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# initialize a classifier\n",
    "model = EndClassifierModel(\n",
    "    batch_size=batch_size, test_batch_size=test_batch_size\n",
    ")\n",
    "\n",
    "# fit it on the training data + majority vote hard labels\n",
    "model.fit(\n",
    "    dataset_train=train_data, \n",
    "    y_train=hard_label_mv, \n",
    "    dataset_valid=valid_data, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# test on the test set\n",
    "model.test(dataset=test_data, metric_fn=\"acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier with FABLE hard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:10:32.400286Z",
     "start_time": "2023-09-26T15:10:27.930583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.836"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# initialize a classifier\n",
    "model = EndClassifierModel(\n",
    "    batch_size=batch_size, test_batch_size=test_batch_size\n",
    ")\n",
    "\n",
    "# fit it \n",
    "model.fit(\n",
    "    dataset_train=train_data, \n",
    "    y_train=hard_label_fable, \n",
    "    dataset_valid=valid_data,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# test on the test set\n",
    "model.test(dataset=test_data, metric_fn=\"acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-2-End training with SepLL\n",
    "\n",
    "In the following, we use a state-of-the-art method called SepLL [4] to train a classifier with weak labels. During training, LF matches are the only training signal, and prediction is then later made from a latent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T15:12:00.176454Z",
     "start_time": "2023-09-26T15:11:29.522834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-28 23:35:08 - \n",
      "==========[hyper parameters]==========\n",
      "{\n",
      "    \"batch_size\": 32,\n",
      "    \"real_batch_size\": 16,\n",
      "    \"test_batch_size\": 32,\n",
      "    \"n_steps\": 10000,\n",
      "    \"grad_norm\": -1,\n",
      "    \"use_lr_scheduler\": false,\n",
      "    \"binary_mode\": false\n",
      "}\n",
      "==========[optimizer config]==========\n",
      "{\n",
      "    \"name\": \"Adam\",\n",
      "    \"paras\": {\n",
      "        \"lr\": 0.001,\n",
      "        \"weight_decay\": 0.0\n",
      "    }\n",
      "}\n",
      "==========[backbone config]==========\n",
      "{\n",
      "    \"name\": \"MLP\",\n",
      "    \"paras\": {\n",
      "        \"hidden_size\": 100,\n",
      "        \"dropout\": 0.0,\n",
      "        \"model_name\": \"roberta-base\"\n",
      "    }\n",
      "}\n",
      "==========[label model_config config]==========\n",
      "{\n",
      "    \"name\": \"MajorityVoting\",\n",
      "    \"paras\": {}\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../wrench/dataset/utils.py:43: UserWarning: No unlabeled data found! Use full dataset us unlabeled dataset\n",
      "  warnings.warn('No unlabeled data found! Use full dataset us unlabeled dataset')\n",
      "../wrench/classification/sepll.py:239: RuntimeWarning: invalid value encountered in divide\n",
      "  labeled_dataset.weak_labels / labeled_dataset.weak_labels.sum(axis=1, keepdims=True)\n",
      "../wrench/classification/sepll.py:243: RuntimeWarning: invalid value encountered in divide\n",
      "  dataset_valid.weak_labels / dataset_valid.weak_labels.sum(axis=1, keepdims=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f074109e7b4ae482d41a497c69cc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[TRAIN] SepLL:   0%|                                                                                          …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-28 23:35:38 - [INFO] early stop @ step 3000!\n",
      "2023-09-28 23:35:38 - SepLL test acc: 0.896\n"
     ]
    }
   ],
   "source": [
    "from wrench.classification.sepll import SepLL\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "bert_model_name = 'roberta-base'\n",
    "\n",
    "#### Initialize SepLL\n",
    "model = SepLL(\n",
    "    batch_size=batch_size,\n",
    "    test_batch_size=test_batch_size,\n",
    "    backbone='MLP',\n",
    "    backbone_model_name=bert_model_name,\n",
    "    # \n",
    "    # SepLL specific\n",
    "    add_unlabeled=False,\n",
    "    class_noise=0.0,\n",
    "    lf_l2_regularization=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    dataset_train=train_data,\n",
    "    dataset_valid=valid_data,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "acc = model.test(test_data, 'acc')\n",
    "\n",
    "logger.info(f'SepLL test acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### GPU training\n",
    "\n",
    "In case your environment has a GPU available, it is also possible to make use of the full strength of SepLL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-28 23:35:41 - \n",
      "==========[hyper parameters]==========\n",
      "{\n",
      "    \"batch_size\": 16,\n",
      "    \"real_batch_size\": 16,\n",
      "    \"test_batch_size\": 32,\n",
      "    \"n_steps\": 10000,\n",
      "    \"grad_norm\": -1,\n",
      "    \"use_lr_scheduler\": false,\n",
      "    \"binary_mode\": false\n",
      "}\n",
      "==========[optimizer config]==========\n",
      "{\n",
      "    \"name\": \"Adam\",\n",
      "    \"paras\": {\n",
      "        \"lr\": 5e-05,\n",
      "        \"weight_decay\": 0.0\n",
      "    }\n",
      "}\n",
      "==========[backbone config]==========\n",
      "{\n",
      "    \"name\": \"BERT\",\n",
      "    \"paras\": {\n",
      "        \"model_name\": \"roberta-base\",\n",
      "        \"max_tokens\": 512,\n",
      "        \"fine_tune_layers\": -1\n",
      "    }\n",
      "}\n",
      "==========[label model_config config]==========\n",
      "{\n",
      "    \"name\": \"MajorityVoting\",\n",
      "    \"paras\": {}\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../wrench/dataset/utils.py:43: UserWarning: No unlabeled data found! Use full dataset us unlabeled dataset\n",
      "  warnings.warn('No unlabeled data found! Use full dataset us unlabeled dataset')\n",
      "../wrench/classification/sepll.py:239: RuntimeWarning: invalid value encountered in divide\n",
      "  labeled_dataset.weak_labels / labeled_dataset.weak_labels.sum(axis=1, keepdims=True)\n",
      "../wrench/classification/sepll.py:243: RuntimeWarning: invalid value encountered in divide\n",
      "  dataset_valid.weak_labels / dataset_valid.weak_labels.sum(axis=1, keepdims=True)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9e86c63f264d5c942dc4095dc3995c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[TRAIN] SepLL:   0%|                                                                                          …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-28 23:41:52 - KeyboardInterrupt! do not terminate the process in case need to save the best model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 27\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m#### Initialize SepLL\u001B[39;00m\n\u001B[1;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m SepLL(\n\u001B[1;32m     10\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m     11\u001B[0m     real_batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m     lf_l2_regularization\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m,\n\u001B[1;32m     24\u001B[0m )\n\u001B[0;32m---> 27\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_valid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalid_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43macc\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m     32\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/wrench/classification/sepll.py:349\u001B[0m, in \u001B[0;36mSepLL.fit\u001B[0;34m(self, dataset_train, dataset_valid, y_valid, cut_tied, valid_mode, evaluation_step, metric, direction, patience, tolerance, device, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m    347\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKeyboardInterrupt! do not terminate the process in case need to save the best model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 349\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_finalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m history\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/wrench/basemodel.py:218\u001B[0m, in \u001B[0;36mBaseTorchModel._finalize\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_finalize\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalid_flag:\n\u001B[0;32m--> 218\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_model\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalid_dataloader\n",
      "File \u001B[0;32m~/PycharmProjects/WS_tutorial/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1379\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;66;03m# copy state_dict so _load_from_state_dict can modify it\u001B[39;00m\n\u001B[1;32m   1378\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(state_dict, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_metadata\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1379\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mstate_dict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m()\n\u001B[1;32m   1380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m metadata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1381\u001B[0m     \u001B[38;5;66;03m# mypy isn't aware that \"_metadata\" exists in state_dict\u001B[39;00m\n\u001B[1;32m   1382\u001B[0m     state_dict\u001B[38;5;241m.\u001B[39m_metadata \u001B[38;5;241m=\u001B[39m metadata  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "from wrench.classification.sepll import SepLL\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "batch_size=16\n",
    "bert_model_name = 'roberta-base'\n",
    "\n",
    "#### Initialize SepLL\n",
    "model = SepLL(\n",
    "    batch_size=batch_size,\n",
    "    real_batch_size=batch_size,\n",
    "    test_batch_size=test_batch_size,\n",
    "    # BERT specific parameters\n",
    "    backbone='BERT',\n",
    "    backbone_model_name=bert_model_name,\n",
    "    optimizer='Adam',\n",
    "    optimizer_lr=5e-5,\n",
    "    optimizer_weight_decay=0.0,\n",
    "    \n",
    "    # SepLL specific\n",
    "    add_unlabeled=False,\n",
    "    class_noise=0.0,\n",
    "    lf_l2_regularization=0.5,\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    dataset_train=train_data,\n",
    "    dataset_valid=valid_data,\n",
    "    metric='acc',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "acc = model.test(test_data, 'acc')\n",
    "\n",
    "logger.info(f'SepLL test acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Zhang et al. 2023. Leveraging Instance Features for Label Aggregation in Programmatic Weak Supervision. https://arxiv.org/abs/2210.02724 \n",
    "2. Zhang et al. 2021 WRENCH: A Comprehensive Benchmark for Weak Supervision. https://arxiv.org/abs/2109.11377\n",
    "3. Alberto TC et al.  2015. Tubespam: Comment Spam Filtering on Youtube. https://ieeexplore.ieee.org/document/7424299\n",
    "4. Stephan et al. 2022. SepLL: Separating Latent Class Labels from Weak Supervision Noise. https://arxiv.org/abs/2210.13898\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-tutorial",
   "language": "python",
   "name": "ws-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
