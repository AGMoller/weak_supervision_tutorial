{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9541d5-d3a0-4256-a8cf-8fa5a4fca705",
   "metadata": {},
   "source": [
    "# Part 3: Weakly supervised part-of-speech tagging\n",
    "\n",
    "In this part, we will work on a different type of tasks, which is called sequence labeling. Instead of having one label for an entire text, in sequence labeling, we assign a label to each token in the text.\n",
    "Specifically we chose Part-of-speech (POS) tagging, which concerns the task of assigning a POS tag that indicates a grammatical type, to a word based on its definition and context.\n",
    "\n",
    "In order to perform weakly supervised POS tagging, we will employ the [skweak toolkit](https://github.com/NorskRegnesentral/skweak).\n",
    "We will create labeling functions to assign POS tags based on syntactic analysis and grammatical rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:19:48.192042Z",
     "start_time": "2023-09-26T14:19:48.164290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Corpus\n",
    "\n",
    "import skweak\n",
    "\n",
    "from scripts.skweak_ner_eval import evaluate\n",
    "from scripts.utils import load_data_split, get_frequent_words, tag_all\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b562f0c-c762-44c3-ab5e-c8d1e53227ee",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We will use the [English corpus](https://universaldependencies.org/treebanks/en_ewt/index.html) from Universal Dependencies, a framework that contains consistent grammatical annotations across many different languages.\n",
    "The texts in the corpus come from five types of web media: weblogs, newsgroups, emails, reviews, and Yahoo! answers and consist of 254,825 words and 16,621 sentences.\n",
    "\n",
    "Skweak operates on spaCy ``doc`` objects, so the dataset is loaded in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5384a7d18fdb0983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:15:16.042438Z",
     "start_time": "2023-09-26T14:15:11.406722Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Path to the dataset file\n",
    "data_path = os.path.join(\"corpus\", \"UD_English-EWT\")\n",
    "\n",
    "# Create a blank spacy pipeline\n",
    "nlp = spacy.blank(\"xx\")\n",
    "\n",
    "# Load training data\n",
    "reader = Corpus(os.path.join(data_path, \"train.spacy\"))\n",
    "train_data = list(reader(nlp))\n",
    "\n",
    "# Load test data\n",
    "reader = Corpus(os.path.join(data_path, \"test.spacy\"))\n",
    "test_data = list(reader(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b466a7-446d-41e5-b7f9-36f8e5883b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:15:16.887419Z",
     "start_time": "2023-09-26T14:15:16.599843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12543 documents in the training set\n",
      "There are 2077 documents in the test set\n"
     ]
    }
   ],
   "source": [
    "# Get the doc objects\n",
    "train_docs = [doc.reference.copy() for doc in train_data]\n",
    "print(\"There are\", len(train_docs), \"documents in the training set\")\n",
    "test_docs = [doc.reference.copy() for doc in test_data]\n",
    "print(\"There are\", len(test_docs), \"documents in the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd374d-47a9-4d0a-abac-316250269587",
   "metadata": {},
   "source": [
    "## Part-of-speech (POS) tagging\n",
    "\n",
    "The goal is to assign a POS tag to each token.\n",
    "\n",
    "For this tutorial, we will use the following subset of the [universal POS tags](https://universaldependencies.org/u/pos/index.html):\n",
    "1. **DET**: determiner, which is a word that modifies nouns or noun phrases and expresses the reference of the noun phrase in context.\n",
    "2. **NUM**: numeral. It is a word that expresses a number and a relation to the number, such as quantity, sequence, frequency or fraction.\n",
    "3. **PROPN**: proper noun is a noun that is the name of a specific individual, place, or object.\n",
    "4. **ADJ**: adjective, which is a word that typically modifies nouns and specifies their properties or attributes.\n",
    "5. **NOUN**: noun, which is a part of speech typically denoting a person, place, thing, animal or idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5a21f417219cacde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:41:56.268643Z",
     "start_time": "2023-09-26T14:41:56.097172Z"
    }
   },
   "outputs": [],
   "source": [
    "all_labels = [\"DET\",  \"PROPN\", \"ADJ\", \"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a47411-b9c9-4e0d-a9c4-b957d628c2a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:15:21.550402Z",
     "start_time": "2023-09-26T14:15:21.490296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that assigns the gold labels based on the subset we chose\n",
    "def assign_gold_labels(docs):\n",
    "    for doc in docs:\n",
    "        # print([s.text for s in doc.sents])\n",
    "        ents = []\n",
    "        tok_pos = []\n",
    "        for tok in doc:\n",
    "            if tok.pos_ in all_labels:\n",
    "                # print(tok.pos_)\n",
    "                tok_pos.append(tok.pos_)\n",
    "                ents.append(Span(doc, tok.i, tok.i + 1, tok.pos_))\n",
    "            else:\n",
    "                tok_pos.append(\"O\")\n",
    "        doc.set_ents(ents)\n",
    "        # print(tok_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1511daf8-b132-4e61-b214-fcaaf3366887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T13:26:51.443776Z",
     "start_time": "2023-09-26T13:26:51.053393Z"
    }
   },
   "outputs": [],
   "source": [
    "assign_gold_labels(train_docs)\n",
    "assign_gold_labels(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbfa7b-9fbf-4c26-9512-f92aa2b35c02",
   "metadata": {},
   "source": [
    "## 3.1 Labeling functions\n",
    "\n",
    "In the first step, we find the 200 most frequent words in our training corpus and use a lexicon to label these words. In the second step, we mannually annotate the 50 most frequent words.\n",
    "Finally, for each POS tag we will create the following labeling functions: \n",
    "\n",
    "*   DET --> Lexicon with determiners.\n",
    "*   NUM --> If the token is a number.\n",
    "*   PROPN --> A word that is capitalized.\n",
    "*   ADJ --> Suffixes: “able”, “al”, “ful”, “ic”, “ive”, “less”, “ous”, ”y”, “ish”, “ible”, \"est\".\n",
    "*   NOUN --> 1. Suffixes: \"ment\", \"tion\", \"sion\", \"xion\", \"ant\", \"ent\", \"ee\", \"er\", \"or\", \"ism\", \"ist\", \"ness\", \"ship\", \"ity\", \"ance\", \"ence\", \"ar\", \"or\", \"y\", \"acy\", \"age\" , 2. Linguistic rule: if the previous word is a DET, a NUM or an ADJ, then the current one is a NOUN.\n",
    "*   VERB --> 1. Suffixes: \"ing\", \"ate\", \"en\", \"ed\", \"ify\", \"ise\", \"ize\", 2. Linguistic rule: if the previous word is a NOUN, then the current one is a VERB, 3. Previous word is a form of \"be\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071eda5-6faf-43dc-ad4f-169aaa94e251",
   "metadata": {},
   "source": [
    "#### Lexicon LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb3ff73f-4151-4abf-975a-8139ceb7a10f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:25.915525Z",
     "start_time": "2023-09-26T14:18:17.009535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", \"n't\", 'would', 'one', 'like']\n"
     ]
    }
   ],
   "source": [
    "common_words = get_frequent_words(train_docs, 200)\n",
    "print(common_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e20d614-94d6-4f45-a006-f1e87a6f9361",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:25.962211Z",
     "start_time": "2023-09-26T14:18:25.915978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the lexicon\n",
    "with open(\"noun_vb_adj_list.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Create a dictionary with the words and their pos tags\n",
    "lexicon = {}\n",
    "for l in lines:\n",
    "    values = l.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "    lexicon[values[0]] = values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ad9db7c-35f2-488a-b0e7-c18e0aa94833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:25.977929Z",
     "start_time": "2023-09-26T14:18:25.940093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3387 words in the lexicon.\n",
      "[('people', 'NOUN'), ('history', 'NOUN'), ('way', 'NOUN'), ('art', 'NOUN'), ('world', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(lexicon), \"words in the lexicon.\")\n",
    "print(list(lexicon.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db38ff17-3440-46f3-a017-ec7a0823c7ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:26.034365Z",
     "start_time": "2023-09-26T14:18:25.971454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many of the common words we found exist in the lexicon\n",
    "len((list(set(common_words) & set(list(lexicon.keys())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722aefef-ed00-4c25-bf29-0d39a6db0701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:01.245495Z",
     "start_time": "2023-09-26T14:18:01.208213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lexicon LF\n",
    "def common_word_detector(doc):\n",
    "    for token in doc:\n",
    "        # If the frequent word exists in the lexicon use its assigned pos tag\n",
    "        if token.text.lower() in common_words and token.text.lower() in list(lexicon.keys()):\n",
    "            yield token.i, token.i + 1, lexicon[token.text.lower()]\n",
    "\n",
    "\n",
    "word_lf = skweak.heuristics.FunctionAnnotator(\"common_words\", common_word_detector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252eb22-0fa6-4e01-b8a3-f28fdc909314",
   "metadata": {},
   "source": [
    "#### Manual annotation LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17544d20-5048-490f-afd8-b1a6b96e53a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:17.013817Z",
     "start_time": "2023-09-26T14:18:07.394416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", \"n't\", 'would', 'one', 'like', 'time', 'get', 'know', 'also', 'us', 'good', 'could', 'new', 'go', 'please', '$', 'people', 'may', 'back', 'said', 'even', 'work', 'bush', 'well', 'want', 'great', 'way', 'see', 'best', 'place', 'take', \"'m\", 'going', 'service', 'need', 'thanks', 'make', 'many', 'year', 'number', 'day', 'two', 'think', 'much', 'food', 'let', 'first', 'call', '2', 'help']\n"
     ]
    }
   ],
   "source": [
    "# Manual annotation\n",
    "top50_words = get_frequent_words(train_docs, 50)\n",
    "print(top50_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bc491a1-0a9f-4a6f-88dd-76b4c65f1f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:26.049606Z",
     "start_time": "2023-09-26T14:18:25.990349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Annotate the words that belong to our chosen subset\n",
    "manual_tags = {\n",
    "    \"one\": \"NUM\",\n",
    "    \"like\": \"VERB\",\n",
    "    \"time\": \"NOUN\",\n",
    "    \"get\": \"VERB\",\n",
    "    \"know\": \"VERB\",\n",
    "    \"good\": \"ADJ\",\n",
    "    \"could\": \"VERB\",\n",
    "    \"new\": \"ADJ\",\n",
    "    \"go\": \"VERB\",\n",
    "    \"please\": \"VERB\",\n",
    "    \"people\": \"NOUN\",\n",
    "    \"said\": \"VERB\",\n",
    "    \"work\": \"VERB\",\n",
    "    \"bush\": \"NOUN\",\n",
    "    \"want\": \"VERB\",\n",
    "    \"great\": \"ADJ\",\n",
    "    \"way\": \"NOUN\",\n",
    "    \"see\": \"VERB\",\n",
    "    \"best\": \"ADJ\",\n",
    "    \"place\": \"NOUN\",\n",
    "    \"take\": \"VERB\",\n",
    "    \"going\": \"VERB\",\n",
    "    \"service\": \"NOUN\",\n",
    "    \"need\": \"VERB\",\n",
    "    \"make\": \"VERB\",\n",
    "    \"year\": \"NOUN\",\n",
    "    \"number\": \"NOUN\",\n",
    "    \"day\": \"NOUN\",\n",
    "    \"two\": \"NUM\",\n",
    "    \"think\": \"VERB\",\n",
    "    \"food\": \"NOUN\",\n",
    "    \"let\": \"VERB\",\n",
    "    \"first\": \"ADJ\",\n",
    "    \"call\": \"VERB\",\n",
    "    \"2\": \"NUM\",\n",
    "    \"help\": \"VERB\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9558b408-4ef9-4a2f-a3aa-fa22532b2f19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:26.306276Z",
     "start_time": "2023-09-26T14:18:26.257987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manual POS tags LF\n",
    "def manual_pos_tagger(doc):\n",
    "    for token in doc:\n",
    "        if token.text.lower() in manual_tags:\n",
    "            yield token.i, token.i + 1, manual_tags[token.text.lower()]\n",
    "\n",
    "\n",
    "manual_pos_lf = skweak.heuristics.FunctionAnnotator(\"manual_pos\", manual_pos_tagger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d654a-0f2e-4f7c-9b72-1a790de0e564",
   "metadata": {},
   "source": [
    "#### DET LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36f5808a-053e-4d82-87b7-c7a58d776087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:28.306017Z",
     "start_time": "2023-09-26T14:18:28.255451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from det.json\n",
      "Populating trie for class DET (number: 47)\n"
     ]
    }
   ],
   "source": [
    "# Use a lexicon of determiners\n",
    "tries = skweak.gazetteers.extract_json_data(\"det.json\")\n",
    "det_lf = skweak.gazetteers.GazetteerAnnotator(\"determiners\", tries, case_sensitive=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7533e-b027-4753-8eae-03df346efa67",
   "metadata": {},
   "source": [
    "#### NUM LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eeb92f1-b25d-4f8f-8d13-73ede592fa65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:30.672033Z",
     "start_time": "2023-09-26T14:18:30.622023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use a regular expression pattern to look for digits\n",
    "def num_detector(doc):\n",
    "    for token in doc:\n",
    "        if re.search(\"\\d+\", token.text):\n",
    "            yield token.i, token.i + 1, \"NUM\"\n",
    "\n",
    "\n",
    "num_lf = skweak.heuristics.FunctionAnnotator(\"numerals\", num_detector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6c463-33dc-45aa-a4c3-e206f01a1558",
   "metadata": {},
   "source": [
    "#### PROPN LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87790367-e082-4a2d-a794-24ed64ac55a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:32.100409Z",
     "start_time": "2023-09-26T14:18:32.043339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the fist letter of a word is capitalized\n",
    "def propn_detector(doc):\n",
    "    for token in doc:\n",
    "        if token.i == 0:\n",
    "            # For the first word of a sentence, check if all letters are capitalized\n",
    "            if token.text.isupper():\n",
    "                yield token.i, token.i + 1, \"PROPN\"\n",
    "        else:\n",
    "            if token.text.isupper() or token.text[0].isupper():\n",
    "                yield token.i, token.i + 1, \"PROPN\"\n",
    "\n",
    "\n",
    "propn_lf = skweak.heuristics.FunctionAnnotator(\"proper_nouns\", propn_detector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8181f-da19-4c11-9636-79f95892025d",
   "metadata": {},
   "source": [
    "#### ADJ LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e24c251-3ec5-49e4-91fb-c1ed20848963",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:33.540656Z",
     "start_time": "2023-09-26T14:18:33.498083Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look for common suffixes and prefixes\n",
    "def adj_detector_suffixes(doc):\n",
    "    suffixes = (\"able\", \"al\", \"ful\", \"ic\", \"ive\", \"less\", \"ous\", \"y\", \"ish\", \"ible\", \"ent\", \"est\")\n",
    "    for token in doc:\n",
    "        if len(token.text) > 3 and token.text.endswith(suffixes):\n",
    "            yield token.i, token.i + 1, \"ADJ\"\n",
    "\n",
    "\n",
    "# Look for common prefixes\n",
    "def adj_detector_prefixes(doc):\n",
    "    prefixes = (\"un\", \"im\", \"in\", \"ir\", \"il\", \"non\", \"dis\")\n",
    "    for token in doc:\n",
    "        if len(token.text) > 3 and token.text.lower().startswith(prefixes):\n",
    "            yield token.i, token.i + 1, \"ADJ\"\n",
    "\n",
    "\n",
    "# If the previous word is a form of \"be\" and the current word does not end with \"ing\" and was not labeled as DET, then it's an adjective\n",
    "def adj_detector(doc):\n",
    "    weak_labels = [\"O\"] * len(doc)\n",
    "    for span in doc.spans[\"determiners\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for token in doc[1:]:\n",
    "        if not token.is_punct:\n",
    "            prev = doc[token.i - 1].text.lower()\n",
    "            if prev in [\"be\", \"been\", \"being\", \"am\", \"is\", \"are\", \"was\", \"were\"] and (\n",
    "                    not token.text.endswith(\"ing\")) and weak_labels[token.i] == \"O\":\n",
    "                yield token.i, token.i + 1, \"ADJ\"\n",
    "\n",
    "\n",
    "# If the previous word is labeld as DET or NUM, then the current word is an adjective\n",
    "def adj_detector_ling(doc):\n",
    "    weak_labels = [\"O\"] * len(doc)\n",
    "\n",
    "    for span in doc.spans[\"determiners\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for span in doc.spans[\"numerals\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for token in doc[1:]:\n",
    "        if not token.is_punct:\n",
    "            if weak_labels[token.i - 1] != \"O\":\n",
    "                yield token.i, token.i + 1, \"ADJ\"\n",
    "\n",
    "\n",
    "adj_lf1 = skweak.heuristics.FunctionAnnotator(\"adjectives1\", adj_detector_suffixes)\n",
    "adj_lf2 = skweak.heuristics.FunctionAnnotator(\"adjectives2\", adj_detector_prefixes)\n",
    "adj_lf3 = skweak.heuristics.FunctionAnnotator(\"adjectives3\", adj_detector)\n",
    "adj_lf4 = skweak.heuristics.FunctionAnnotator(\"adjectives4\", adj_detector_ling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2b5d4-9834-40f8-bad6-b5a347f4a457",
   "metadata": {},
   "source": [
    "#### NOUN LF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25391729-012c-4069-b44f-426ce24f3c2d",
   "metadata": {},
   "source": [
    "Let's create a labeling function that looks for common noun suffixes. Can you think of some?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eceff82-36ec-4ac6-9aeb-64db0b57e573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:36.463325Z",
     "start_time": "2023-09-26T14:18:36.401746Z"
    }
   },
   "outputs": [],
   "source": [
    "# ***********************************\n",
    "def noun_detector_suffixes(doc):\n",
    "    suffixes = (\"ment\", \"tion\", \"sion\", \"xion\", \"ant\", \"ent\", \"ee\", \"er\", \"or\",\n",
    "                \"ism\", \"ist\", \"ness\", \"ship\", \"ity\", \"ance\", \"ence\",\n",
    "                \"ar\", \"or\", \"y\", \"acy\", \"age\")\n",
    "    for token in doc:\n",
    "        if len(token.text) > 3 and token.text.lower().endswith(suffixes):\n",
    "            yield token.i, token.i + 1, \"NOUN\"\n",
    "\n",
    "# ***********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c00243b-f55c-4b29-a3d2-bf6a23494253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:18:38.038128Z",
     "start_time": "2023-09-26T14:18:37.978281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look for common prefixes\n",
    "def noun_detector_prefixes(doc):\n",
    "    prefixes = (\n",
    "        \"anti\", \"auto\", \"bi\", \"co\", \"counter\", \"dis\", \"ex\", \"hyper\", \"in\", \"inter\", \"kilo\", \"mal\", \"mega\", \"mis\",\n",
    "        \"mini\", \"mono\", \"neo\", \"out\", \"poly\", \"pseudo\", \"re\", \"semi\", \"sub\", \"super\", \"sur\", \"tele\", \"tri\", \"ultra\",\n",
    "        \"under\", \"vice\")\n",
    "    for token in doc:\n",
    "        if len(token.text) > 3 and token.text.lower().startswith(prefixes):\n",
    "            yield token.i, token.i + 1, \"NOUN\"\n",
    "\n",
    "\n",
    "# # If the previous word is labeld as DET, NUM or ADJ, then the current word is an noun\n",
    "def noun_detector_ling(doc):\n",
    "    weak_labels = [\"O\"] * len(doc)\n",
    "\n",
    "    for span in doc.spans[\"determiners\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for span in doc.spans[\"numerals\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for span in doc.spans[\"adjectives1\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for span in doc.spans[\"adjectives2\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for span in doc.spans[\"adjectives3\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for span in doc.spans[\"adjectives4\"]:\n",
    "        weak_labels[span.start] = span.label_\n",
    "\n",
    "    for token in doc[1:]:\n",
    "        if not token.is_punct:\n",
    "            if weak_labels[token.i - 1] != \"O\":\n",
    "                yield token.i, token.i + 1, \"NOUN\"\n",
    "\n",
    "\n",
    "noun_lf1 = skweak.heuristics.FunctionAnnotator(\"nouns1\", noun_detector_suffixes)\n",
    "noun_lf2 = skweak.heuristics.FunctionAnnotator(\"nouns2\", noun_detector_prefixes)\n",
    "noun_lf3 = skweak.heuristics.FunctionAnnotator(\"nouns3\", noun_detector_ling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8993d97-1611-4048-8392-4d4abb89e645",
   "metadata": {},
   "source": [
    "## Apply LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c62dd04d953cc2ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:42:57.191550Z",
     "start_time": "2023-09-26T14:42:49.051820Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Put all LFs in a list\n",
    "lfs = [\n",
    "    word_lf, manual_pos_lf,\n",
    "    det_lf, num_lf, propn_lf,\n",
    "    adj_lf1, adj_lf2, adj_lf3, adj_lf4,\n",
    "    noun_lf1, noun_lf2, noun_lf3\n",
    "]\n",
    "\n",
    "train_docs = load_data_split(\"train\", all_labels)\n",
    "train_docs = tag_all(train_docs, lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a94d611f-e687-4e8e-8636-4b6bdbd818dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:42:57.201488Z",
     "start_time": "2023-09-26T14:42:57.190526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Al-Zaman : American forces killed Shaikh Abdullah al-Ani, \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DET</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    preacher\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NOUN</span>\n",
       "</mark>\n",
       " at \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DET</span>\n",
       "</mark>\n",
       " mosque in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DET</span>\n",
       "</mark>\n",
       " town of Qaim, \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    near\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NOUN</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DET</span>\n",
       "</mark>\n",
       " Syrian \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    border\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NOUN</span>\n",
       "</mark>\n",
       ". </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">[\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    This\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DET</span>\n",
       "</mark>\n",
       " killing of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    a\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DET</span>\n",
       "</mark>\n",
       " respected cleric will be causing us trouble for years to come.] </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">DPA: Iraqi authorities announced \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    that\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DET</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    they\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NOUN</span>\n",
       "</mark>\n",
       " had busted up 3 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    terrorist\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NOUN</span>\n",
       "</mark>\n",
       " cells operating in Baghdad. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print some of the assigned weak labels\n",
    "for doc in train_docs[0:3]:\n",
    "    skweak.utils.display_entities(doc, [\"determiners\", \"nouns1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "41c269d6-1a2d-4ac6-8a2b-b78ce149f99b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:37.538756Z",
     "start_time": "2023-09-26T14:42:57.201082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Finished E-step with 12543 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -481878.0229             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Finished E-step with 12543 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2     -454753.3161      +27124.7068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Finished E-step with 12543 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3     -440820.3014      +13933.0147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Finished E-step with 12543 documents\n",
      "Starting iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4     -433598.0648       +7222.2366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Finished E-step with 12543 documents\n",
      "Starting iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         5     -429716.1331       +3881.9317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Finished E-step with 12543 documents\n",
      "Starting iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         6     -426856.2936       +2859.8395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Finished E-step with 12543 documents\n",
      "Starting iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         7     -423013.7484       +3842.5452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Finished E-step with 12543 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         8     -417588.7557       +5424.9927\n"
     ]
    }
   ],
   "source": [
    "# Train HMM\n",
    "hmm = skweak.aggregation.HMM(\"hmm\", all_labels)\n",
    "hmm = hmm.fit(train_docs, n_iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cc0fe7d6-081c-4e01-96b5-45da566cbb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:37.541616Z",
     "start_time": "2023-09-26T14:44:37.538165Z"
    }
   },
   "outputs": [],
   "source": [
    "# Majority voting\n",
    "mv = skweak.aggregation.MajorityVoter(\"mv\", all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9276ba55-6b54-4447-8d74-b14c3c3376c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:41.917675Z",
     "start_time": "2023-09-26T14:44:37.538646Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Apply LFs, HMM and MV to the test docs\n",
    "test_docs = load_data_split(\"test\", all_labels)\n",
    "test_docs = tag_all(test_docs, lfs + [mv, hmm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408db9c-0773-4cbb-9d38-428fb4d66c52",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c729c8f7-ff96-4e68-ad85-736de15fafcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:43.982941Z",
     "start_time": "2023-09-26T14:44:41.920435Z"
    }
   },
   "outputs": [],
   "source": [
    "df = evaluate(test_docs, all_labels, [\n",
    "    # \"common_words\", \"manual_pos\",\n",
    "    # \"determiners\", \"numerals\", \"proper_nouns\",\n",
    "    # \"adjectives1\", \"adjectives2\", \"adjectives3\", \"adjectives4\",\n",
    "    # \"nouns1\", \"nouns2\", \"nouns3\", \n",
    "    \"mv\", \"hmm\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d08645df53999d7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:44.020465Z",
     "start_time": "2023-09-26T14:44:43.985791Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tok_precision</th>\n",
       "      <th>tok_recall</th>\n",
       "      <th>tok_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>proportion</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ADJ</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">17.3 %</th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.183</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">DET</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">19.4 %</th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.687</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NOUN</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">42.2 %</th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.299</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.346</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PROPN</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">21.2 %</th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.623</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.576</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">macro</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.448</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.499</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">micro</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.404</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.467</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">weighted</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.423</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.468</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           tok_precision  tok_recall  tok_f1\n",
       "label    proportion model                                   \n",
       "ADJ      17.3 %     hmm            0.183       0.323   0.234\n",
       "                    mv             0.367       0.389   0.378\n",
       "DET      19.4 %     hmm            0.687       0.912   0.784\n",
       "                    mv             0.708       0.845   0.770\n",
       "NOUN     42.2 %     hmm            0.299       0.176   0.222\n",
       "                    mv             0.346       0.386   0.364\n",
       "PROPN    21.2 %     hmm            0.623       0.279   0.386\n",
       "                    mv             0.576       0.482   0.524\n",
       "macro               hmm            0.448       0.422   0.434\n",
       "                    mv             0.499       0.526   0.512\n",
       "micro               hmm            0.404       0.366   0.384\n",
       "                    mv             0.467       0.496   0.482\n",
       "weighted            hmm            0.423       0.366   0.392\n",
       "                    mv             0.468       0.496   0.482"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a0d51-ea4e-4e64-8eae-1c8f832fe6df",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "\n",
    "* We see that POS tags like determiners and numerals are easier to detect and we can achieve a very good F1 score with just one simple LF.\n",
    "* Other POS tags like adjectives and nouns, which rely more on the context are harder to detect and require more complicated rules.\n",
    "* For adjectives the LF that uses suffixes works the best, while the syntactic rules are less accurate. On the contrary, for nouns the LF that is based on syntactic analysis has the best results. For both POS tags, the LFs that use prefixes do not yield good results.\n",
    "* Despite its simplicity, majority voting outperforms HMM on most of the POS tags and overall achieves a higher macro F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bfa6d-a276-4855-976c-b9e43d1528eb",
   "metadata": {},
   "source": [
    "## Run LFs on a smaller subset\n",
    "\n",
    "What if we use a smaller amount of training data? How will that affect the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "82e0c22349a19a76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:47.324937Z",
     "start_time": "2023-09-26T14:44:44.008815Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_train_docs = load_data_split(\"train\", all_labels, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd2159-64e8-4294-a218-e55823d3bf9e",
   "metadata": {},
   "source": [
    "#### Annotate the most common 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a8344f49-22f7-4bda-b3c4-5a229dd63d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:47.852830Z",
     "start_time": "2023-09-26T14:44:47.326499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", 'bush', 'al', 'india', 'would', 'iraq', 'us', 'iraqi', \"n't\", 'one', 'many', 'even', 'indian', 'said', 'new', 'war', 'musharraf', 'peace', 'years', 'country', 'military', 'israel', 'two', 'also', 'national', 'time', 'chernobyl', 'pakistan', 'government', 'kashmir', 'sri', 'elections', 'know', 'qaeda', 'may', 'president', 'power', 'last', 'another', 'lanka', 'posada', 'back', 'could', 'state', 'general', 'made', 'much', 'party', 'united', 'people']\n"
     ]
    }
   ],
   "source": [
    "# Manual annotation\n",
    "top50_words = get_frequent_words(subset_train_docs, 50)\n",
    "print(top50_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7bc910f7-3f5a-490f-80bb-dc22edb99902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:47.870043Z",
     "start_time": "2023-09-26T14:44:47.854385Z"
    }
   },
   "outputs": [],
   "source": [
    "manual_tags = {\n",
    "    \"bush\": \"NOUN\",\n",
    "    \"al\": \"PROPN\",\n",
    "    \"india\": \"PROPN\",\n",
    "    \"iraq\": \"PROPN\",\n",
    "    \"iraqi\": \"ADJ\",\n",
    "    \"indian\": \"ADJ\",\n",
    "    \"said\": \"VERB\",\n",
    "    \"new\": \"ADJ\",\n",
    "    \"war\": \"NOUN\",\n",
    "    \"musharraf\": \"PROPN\",\n",
    "    \"peace\": \"NOUN\",\n",
    "    \"years\": \"NOUN\",\n",
    "    \"country\": \"NOUN\",\n",
    "    \"military\": \"NOUN\",\n",
    "    \"israel\": \"PROPN\",\n",
    "    \"two\": \"NUM\",\n",
    "    \"national\": \"ADJ\",\n",
    "    \"time\": \"NOUN\",\n",
    "    \"chernobyl\": \"PROPN\",\n",
    "    \"pakistan\": \"PROPN\",\n",
    "    \"government\": \"NOUN\",\n",
    "    \"kashmir\": \"PROPN\",\n",
    "    \"sri\": \"PROPN\",\n",
    "    \"elections\": \"NOUN\",\n",
    "    \"know\": \"VERB\",\n",
    "    \"qaeda\": \"PROPN\",\n",
    "    \"president\": \"NOUN\",\n",
    "    \"power\": \"NOUN\",\n",
    "    \"last\": \"NOUN\",\n",
    "    \"another\": \"ADJ\",\n",
    "    \"lanka\": \"PROPN\",\n",
    "    \"posada\": \"PROPN\",\n",
    "    \"could\": \"VERB\",\n",
    "    \"general\": \"ADJ\",\n",
    "    \"made\": \"VERB\",\n",
    "    \"party\": \"NOUN\",\n",
    "    \"united\": \"VERB\",\n",
    "    \"people\": \"NOUN\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1925d91d-e725-41c7-ba12-97481da4f0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:48.141961Z",
     "start_time": "2023-09-26T14:44:47.873354Z"
    }
   },
   "outputs": [],
   "source": [
    "# ***********************************\n",
    "\n",
    "# Apply LFs to the subset docs\n",
    "for doc in subset_train_docs:\n",
    "    for lf in lfs:\n",
    "        doc = lf(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "265d154f-6977-4fee-b4ad-de485dbf44f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:53.160902Z",
     "start_time": "2023-09-26T14:44:48.141679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 500 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1      -33070.3558             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 500 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2      -31391.1937       +1679.1620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 500 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3      -30545.1126        +846.0811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 500 documents\n",
      "Starting iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4      -29878.0429        +667.0697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 500 documents\n",
      "Starting iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         5      -29287.5158        +590.5271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 500 documents\n",
      "Starting iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         6      -28811.0492        +476.4665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 500 documents\n",
      "Starting iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         7      -28586.8415        +224.2078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 500 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         8      -28536.3379         +50.5036\n"
     ]
    }
   ],
   "source": [
    "# Train HMM\n",
    "hmm_2 = skweak.aggregation.HMM(\"hmm_2\", all_labels, redundancy_factor=1.0)\n",
    "hmm_2 = hmm_2.fit(subset_train_docs, n_iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bea8f19b-a1d5-4bec-a66d-214d1519454f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:53.175660Z",
     "start_time": "2023-09-26T14:44:53.162053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Majority voting\n",
    "mv = skweak.aggregation.MajorityVoter(\"mv\", all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "15b5c522-7ba0-473d-8a74-c2cd23b127ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:57.406495Z",
     "start_time": "2023-09-26T14:44:53.178664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply LFs, HMM and MV to the test docs\n",
    "\n",
    "test_docs_2 = load_data_split(\"test\", all_labels)\n",
    "\n",
    "for doc in test_docs_2:\n",
    "    for lf in  lfs + [mv, hmm_2]:\n",
    "        doc = lf(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9a88bc82-9deb-4d28-bf7c-247fc2a3d401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:59.505580Z",
     "start_time": "2023-09-26T14:44:57.409216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "subset_df = evaluate(test_docs_2, all_labels, [\n",
    "    # \"common_words\", \"manual_pos\",\n",
    "    # \"determiners\", \"numerals\", \"proper_nouns\",\n",
    "    # \"adjectives1\", \"adjectives2\", \"adjectives3\", \"adjectives4\",\n",
    "    # \"nouns1\", \"nouns2\", \"nouns3\", \n",
    "    \"mv\", \"hmm_2\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "200054a82750110c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:59.509120Z",
     "start_time": "2023-09-26T14:44:59.505388Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tok_precision</th>\n",
       "      <th>tok_recall</th>\n",
       "      <th>tok_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>proportion</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ADJ</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">17.3 %</th>\n",
       "      <th>hmm_2</th>\n",
       "      <td>0.185</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">DET</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">19.4 %</th>\n",
       "      <th>hmm_2</th>\n",
       "      <td>0.688</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NOUN</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">42.2 %</th>\n",
       "      <th>hmm_2</th>\n",
       "      <td>0.321</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.346</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PROPN</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">21.2 %</th>\n",
       "      <th>hmm_2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.576</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">macro</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm_2</th>\n",
       "      <td>0.448</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.499</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">micro</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm_2</th>\n",
       "      <td>0.435</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.467</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">weighted</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm_2</th>\n",
       "      <td>0.428</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.468</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           tok_precision  tok_recall  tok_f1\n",
       "label    proportion model                                   \n",
       "ADJ      17.3 %     hmm_2          0.185       0.331   0.238\n",
       "                    mv             0.367       0.389   0.378\n",
       "DET      19.4 %     hmm_2          0.688       0.963   0.802\n",
       "                    mv             0.708       0.845   0.770\n",
       "NOUN     42.2 %     hmm_2          0.321       0.173   0.224\n",
       "                    mv             0.346       0.386   0.364\n",
       "PROPN    21.2 %     hmm_2          0.600       0.581   0.590\n",
       "                    mv             0.576       0.482   0.524\n",
       "macro               hmm_2          0.448       0.512   0.478\n",
       "                    mv             0.499       0.526   0.512\n",
       "micro               hmm_2          0.435       0.439   0.436\n",
       "                    mv             0.467       0.496   0.482\n",
       "weighted            hmm_2          0.428       0.440   0.434\n",
       "                    mv             0.468       0.496   0.482"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a7e861a3-ac87-4c52-92ca-95e67260d912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:44:59.515301Z",
     "start_time": "2023-09-26T14:44:59.509425Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tok_precision</th>\n",
       "      <th>tok_recall</th>\n",
       "      <th>tok_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>proportion</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ADJ</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">17.3 %</th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.183</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">DET</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">19.4 %</th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.687</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NOUN</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">42.2 %</th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.299</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.346</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PROPN</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">21.2 %</th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.623</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.576</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">macro</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.448</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.499</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">micro</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.404</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.467</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">weighted</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>hmm</th>\n",
       "      <td>0.423</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mv</th>\n",
       "      <td>0.468</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           tok_precision  tok_recall  tok_f1\n",
       "label    proportion model                                   \n",
       "ADJ      17.3 %     hmm            0.183       0.323   0.234\n",
       "                    mv             0.367       0.389   0.378\n",
       "DET      19.4 %     hmm            0.687       0.912   0.784\n",
       "                    mv             0.708       0.845   0.770\n",
       "NOUN     42.2 %     hmm            0.299       0.176   0.222\n",
       "                    mv             0.346       0.386   0.364\n",
       "PROPN    21.2 %     hmm            0.623       0.279   0.386\n",
       "                    mv             0.576       0.482   0.524\n",
       "macro               hmm            0.448       0.422   0.434\n",
       "                    mv             0.499       0.526   0.512\n",
       "micro               hmm            0.404       0.366   0.384\n",
       "                    mv             0.467       0.496   0.482\n",
       "weighted            hmm            0.423       0.366   0.392\n",
       "                    mv             0.468       0.496   0.482"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
